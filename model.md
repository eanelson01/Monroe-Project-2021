## Overview of the Model

### Finding a Model
I based my search for a specific Convolutional Neural Network model on my literature review. Many of the papers I read on the topic of building segmentation talked about similar models throughout. Those included VGG-19, UNet, and more. The one I saw the most of was the UNet, specifically in the two papers I highlighted in my [literature review](litreview.md) earlier. The UNet model seemed then to be the best fit for my goals with this project. Additionally, the literature I read talked about a method called transfer learning. This is where a model or CNN is pre-trained, meaning their weights are already determined and not just random. Usually they are trained on a large dataset to give weights for general classification tasks. With transfer learning, you can further train these weights to the use case of classification that you desire. This method seemed to be the best application for me given my circumstances, so I began looking for a transfer learning model that I could implement into my project and apply to my datasets.

### The Model Itself
As  I mentioned, I landed on using a pre-trained UNet model that I could further train on the images I received from the Inria Dataset from earlier. I searched through papers and videos, finally finding a model to use. The model I chose was a UNet structure from the segmentation-models python package that I learned about from a [YouTube Video](https://www.youtube.com/watch?v=J_XSd_u_Yew&list=PLiHR3eIynOPrAg_1h0oFkArC_WO8bpRGA&index=10) by user _DigitalSreeni_. This is a pretrained model which allows you to pick a backbone structure and the encoder weights. For the backbone, I went with the Resnet 34 structure and used Imagenet weights. Imagenet is a large dataset from Google which has a series of images with their classification, making it perfect for training a model for general classification. I used this model and its weights as a basis for further training and testing on my datasets.

### Training the Model 
For training the model I took inspiration from the YouTube video above. I started by loading in the split training images and masks from the Inria Dataset. These were the smaller images derived from that larger 5000 x 5000 images. I resized the images using the cv2 python package to be 256 x 256. This was so that the pixel size was divisible by 32 which is necessary to be placed into the model. I collected these resized images into a list. For the training images which have black and white pixels, I set all values above 0 to 1 and the rest to 0. This makes it so that the images are in binary rather than in RGB values which can range from 0 to 255. By placing them in binary, they will be more easily be interpreted by the model. Next I split the resized and modified images into training and testing data with the 70 to 30 proportion. This means 70% of the images were placed in training and the remaining 30% were for testing. That testing set allows for validation of the model and assessment of the accuracy. Once I had it split, I applied the same preprocessing to the images that was used for the initial model. I was able to do this by calling a function in the segmentation models python package. Finally, I compiled the model, setting the backbone and encoder weights I mentioned above. Additionally, I used an Adam optimizer, binary crossentropy loss function, and accuracy metric. The loss was binary crossentropy because it is deciding between if a pixel value is a 0 or 1: non-building or building. Once I had the model and images set, I fit the model using the training data. I ran the model for a total of 15 epochs with a batch size of 32. I then assessed the results.
 
#### [Information about the Results](results.md)